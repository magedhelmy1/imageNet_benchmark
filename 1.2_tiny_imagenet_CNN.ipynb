{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> General Info</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "\n",
    "\"\"\"\n",
    "1- Implement a naive CNN on the imagenet data\n",
    "2- Implement Capsules\n",
    "\n",
    "Other info:\n",
    "Stanford prepared the Tiny ImageNet dataset for their CS231n course. \n",
    "The dataset spans 200 image classes with 500 training examples per class. \n",
    "The dataset also has 50 validation and 50 test examples per class.\n",
    "\n",
    "Tiny ImageNet Challenge is a similar challenge as ImageNet with a smaller dataset but\n",
    "less image classes. It contains 200 image classes, a training \n",
    "dataset of 100, 000 images, a validation dataset of 10, 000\n",
    "images, and a test dataset of 10, 000 images. All images are\n",
    "of size 64Ã—64.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Import Training Dataset <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-04T15:59:19.730860Z",
     "iopub.status.busy": "2020-05-04T15:59:19.730860Z",
     "iopub.status.idle": "2020-05-04T15:59:22.551093Z",
     "shell.execute_reply": "2020-05-04T15:59:22.550604Z",
     "shell.execute_reply.started": "2020-05-04T15:59:19.730860Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "import datetime\n",
    "import os\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-04T15:59:22.551597Z",
     "iopub.status.busy": "2020-05-04T15:59:22.551597Z",
     "iopub.status.idle": "2020-05-04T15:59:29.459541Z",
     "shell.execute_reply": "2020-05-04T15:59:29.458778Z",
     "shell.execute_reply.started": "2020-05-04T15:59:22.551597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory= 'tiny-imagenet-200/train/', \n",
    "                                                    target_size=(64, 64), \n",
    "                                                    batch_size=256, \n",
    "                                                    class_mode='categorical', \n",
    "                                                    shuffle=True, \n",
    "                                                    seed=42,\n",
    "                                                    subset =\"training\"\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Examine Train Images </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_batch, y_batch = next(train_generator)\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i in range(8):\n",
    "    sub = fig.add_subplot(2, 4, i + 1)\n",
    "    sub.imshow((x_batch[i,:,:,:]), interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import Validation Dataset <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-04T15:59:29.459776Z",
     "iopub.status.busy": "2020-05-04T15:59:29.459776Z",
     "iopub.status.idle": "2020-05-04T15:59:30.190319Z",
     "shell.execute_reply": "2020-05-04T15:59:30.190319Z",
     "shell.execute_reply.started": "2020-05-04T15:59:29.459776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 validated image filenames belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
    "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
    "\n",
    "valid_datagen  = ImageDataGenerator(\n",
    "    rescale=1./ 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_dataframe(dataframe=val_data, \n",
    "                                                         directory='./tiny-imagenet-200/val/images/', \n",
    "                                                         x_col='File', \n",
    "                                                         y_col='Class', \n",
    "                                                         target_size=(64, 64),\n",
    "                                                         color_mode='rgb', \n",
    "                                                         class_mode='categorical', \n",
    "                                                         batch_size=256, \n",
    "                                                         shuffle=True, \n",
    "                                                         seed=42,\n",
    "                                                        subset =\"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Generic Callbacks </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-04T15:59:30.191317Z",
     "iopub.status.busy": "2020-05-04T15:59:30.191317Z",
     "iopub.status.idle": "2020-05-04T15:59:30.196867Z",
     "shell.execute_reply": "2020-05-04T15:59:30.196169Z",
     "shell.execute_reply.started": "2020-05-04T15:59:30.191317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Early stopping and tensorboard graphs\n",
    "\n",
    "es = EarlyStopping(monitor='accuracy', verbose=1, patience=50)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building and training the Model - Lighter version of VGG inspired Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(64,64,3),\n",
    "                 kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(34, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, \n",
    "                    epochs=5000, \n",
    "                    validation_data=validation_generator, \n",
    "                    callbacks=[es]\n",
    "                             )\n",
    "\n",
    "score = model.evaluate(train_generator)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building and training the Model - Inception </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies parallel convolutional layers with different sized filters (1x1, 3x3 and 5x5) followed by a 3x3 max pooling layer \n",
    "# and the results are then concatentated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_img = Input(shape = (64, 64, 3))\n",
    "\n",
    "# 1x1 conv\n",
    "conv1 = Conv2D(32, (1,1), padding='same', activation='relu')(input_img)\n",
    "\n",
    "# 3x3 conv\n",
    "conv3 = Conv2D(32, (3,3), padding='same', activation='relu')(input_img)\n",
    "\n",
    "# 5x5 conv\n",
    "conv5 = Conv2D(32, (5,5), padding='same', activation='relu')(input_img)\n",
    "\n",
    "# 3x3 max pooling\n",
    "pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n",
    "\n",
    "# concatenate filters, assumes filters/channels last\n",
    "layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "\n",
    "output = Flatten()(layer_out)\n",
    "out    = Dense(34, activation='softmax')(output)\n",
    "\n",
    "model = Model(inputs = input_img, outputs = out)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, \n",
    "                    epochs=2000, \n",
    "                    validation_data=validation_generator, \n",
    "                    callbacks=[es]\n",
    "                             )\n",
    "\n",
    "score = model.evaluate(train_generator)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building and training the Model - Resnet50V2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Network (ResNet) is a Convolutional Neural Network (CNN) architecture which was designed to enable hundreds or thousands of \n",
    "# convolutional layers. While previous CNN architectures had a drop off in the effectiveness of additional layers, \n",
    "# ResNet can add a large number of layers with strong performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-04T15:59:34.044694Z",
     "iopub.status.busy": "2020-05-04T15:59:34.044694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 391 steps, validate for 40 steps\n",
      "Epoch 1/2000\n",
      "391/391 [==============================] - 420s 1s/step - loss: 4.8689 - accuracy: 0.0468 - val_loss: 5.6291 - val_accuracy: 0.0273\n",
      "Epoch 2/2000\n",
      "391/391 [==============================] - 137s 352ms/step - loss: 4.2401 - accuracy: 0.1091 - val_loss: 4.3717 - val_accuracy: 0.0943\n",
      "Epoch 3/2000\n",
      "391/391 [==============================] - 135s 344ms/step - loss: 3.9367 - accuracy: 0.1508 - val_loss: 4.3153 - val_accuracy: 0.1084\n",
      "Epoch 4/2000\n",
      "391/391 [==============================] - 135s 346ms/step - loss: 3.7022 - accuracy: 0.1865 - val_loss: 4.4973 - val_accuracy: 0.1315\n",
      "Epoch 5/2000\n",
      "391/391 [==============================] - 138s 354ms/step - loss: 3.5990 - accuracy: 0.2036 - val_loss: 6.8630 - val_accuracy: 0.0069\n",
      "Epoch 6/2000\n",
      "391/391 [==============================] - 142s 364ms/step - loss: 3.6049 - accuracy: 0.2035 - val_loss: 3.9992 - val_accuracy: 0.1586\n",
      "Epoch 7/2000\n",
      "391/391 [==============================] - 144s 369ms/step - loss: 3.3164 - accuracy: 0.2475 - val_loss: 4.1077 - val_accuracy: 0.1576\n",
      "Epoch 8/2000\n",
      "391/391 [==============================] - 144s 369ms/step - loss: 3.1625 - accuracy: 0.2750 - val_loss: 3.4687 - val_accuracy: 0.2331\n",
      "Epoch 9/2000\n",
      "391/391 [==============================] - 144s 367ms/step - loss: 3.0443 - accuracy: 0.2951 - val_loss: 3.4000 - val_accuracy: 0.2392\n",
      "Epoch 10/2000\n",
      "391/391 [==============================] - 144s 369ms/step - loss: 2.9517 - accuracy: 0.3108 - val_loss: 3.3287 - val_accuracy: 0.2603\n",
      "Epoch 11/2000\n",
      "391/391 [==============================] - 143s 365ms/step - loss: 2.8350 - accuracy: 0.3326 - val_loss: 3.2423 - val_accuracy: 0.2708\n",
      "Epoch 12/2000\n",
      "391/391 [==============================] - 144s 367ms/step - loss: 2.7374 - accuracy: 0.3491 - val_loss: 3.2527 - val_accuracy: 0.2776\n",
      "Epoch 13/2000\n",
      "391/391 [==============================] - 143s 366ms/step - loss: 2.6471 - accuracy: 0.3661 - val_loss: 3.2180 - val_accuracy: 0.2915\n",
      "Epoch 14/2000\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.5574 - accuracy: 0.3822"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "model = ResNet50V2(\n",
    "    include_top=True, weights=None, input_shape=(64,64,3), classes=200\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, \n",
    "                    epochs=2000, \n",
    "                    validation_data=validation_generator, \n",
    "                    callbacks=[es]\n",
    "                             )\n",
    "\n",
    "score = model.evaluate(train_generator)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building and training the Model - Resnet150V2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152V2\n",
    "\n",
    "model_2 = ResNet152V2(\n",
    "    include_top=True, weights=None, input_shape=(64,64,3), classes=200\n",
    ")\n",
    "\n",
    "\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model_2.fit(train_generator, \n",
    "                    epochs=5000, \n",
    "                    validation_data=validation_generator, \n",
    "                    callbacks=[es]\n",
    "                             )\n",
    "\n",
    "score = model_2.evaluate(train_generator)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()\n",
    "plot_model(model_2, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "<h1> Archive </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import re\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# class TinyImageNetLoader(object):\n",
    "#     \"\"\"Loader for images from the the Tiny ImageNet set for images of a specific class.\"\"\"\n",
    "#     def __init__(self, mode, data_path, data_type='float32'):\n",
    "#         # mode = 'train' or 'val';\n",
    "#         # data_path: the relative path of tiniy-imagenet, '../tiny-imagenet-200',\n",
    "#         self.data_path = data_path\n",
    "#         self.data_type = data_type\n",
    "#         with open(self.path('wnids.txt')) as f: # detail info of image\n",
    "#             wnids = f.readlines()\n",
    "#             assert len(wnids) == 200\n",
    "#             wnids = [x.strip() for x in wnids]\n",
    "#             self.wnids = wnids\n",
    "#             self.mode = mode\n",
    "#         images = {}\n",
    "#         if mode == 'val': # load validation set\n",
    "#             with open(self.path('val/val_annotations.txt')) as f:\n",
    "#                 labels = f.readlines()\n",
    "#                 #assert len(labels) == 10000\n",
    "#                 labels = [x.split('\\t')[:2] for x in labels]\n",
    "#                 for image, wnid in labels:\n",
    "#                     #assert wnid in self.wnids\n",
    "#                     #assert image.endswith('.JPEG')\n",
    "#                     images.setdefault(wnid, []).append(data_path + '/val/images/' + image)\n",
    "\n",
    "#                 #assert len(images) == len(wnids)\n",
    "#                 #for wnid in images:\n",
    "#                 #    assert len(images[wnid]) == 50\n",
    "#         if mode == 'train': # load training set\n",
    "#             filenames = glob.glob(data_path + '/train/*/images/*.JPEG')\n",
    "#             for filename in filenames:\n",
    "#                 wnid = re.search(r'n\\d+', filename)\n",
    "#                 #label = str(label_dict[match.group()])\n",
    "#                 #filenames_labels.append((filename, label))\n",
    "#                 images.setdefault(wnid.group(), []).append(filename)\n",
    "#         self.imagefiles_wnid_dict = images\n",
    "#         # print(len(self.imagefiles_wnid_dict))\n",
    "            \n",
    "\n",
    "#     def path(self, *path):\n",
    "#         return os.path.join(self.data_path, *path)\n",
    "\n",
    "#     def load_image(self, filename):\n",
    "#         path = os.path.join(filename)\n",
    "#         image = Image.open(path)\n",
    "#         image = np.asarray(image)\n",
    "#         if image.shape != (64, 64, 3):\n",
    "#             # e.g. grayscale\n",
    "#             return None\n",
    "#         assert image.dtype == np.uint8\n",
    "#         image = image.astype(self.data_type)\n",
    "#         assert image.shape == (64, 64, 3)\n",
    "#         return image\n",
    "\n",
    "#     def load_n_images_all_classes(self, nb):\n",
    "#         # nb: number of samples per class, for validation data, max =50, for train, max=500\n",
    "#         # return: X_data in [n, 64, 64, 3], Y_data in [n, 1]\n",
    "#         X_data=np.ndarray([nb*200,64,64,3], dtype=self.data_type)\n",
    "#         Y_data=np.zeros([nb*200,200], dtype=self.data_type)# one hot encoded\n",
    "#         arr = np.arange(nb*200)\n",
    "#         np.random.shuffle(arr) # shuffle images\n",
    "        \n",
    "#         for label in range(200):\n",
    "#             wnid = self.wnids[label]\n",
    "#             files = self.imagefiles_wnid_dict[wnid]\n",
    "#             for i, filename in enumerate(files):\n",
    "#                 if i==nb:\n",
    "#                     break\n",
    "#                 #print(i,filename)\n",
    "#                 raw_image = self.load_image(filename)\n",
    "#                 #raw_image = raw_image.astype(np.float32)\n",
    "#                 #print(label*nb+i)\n",
    "#                 X_data[arr[label*nb+i],:,:,:] = raw_image\n",
    "#                 Y_data[arr[label*nb+i],label] = 1\n",
    "#                 #print(label)\n",
    "#         return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=\"C:\\\\Users\\\\Maged Helmy\\\\Desktop\\\\Projects\\\\jupyterNotebooks_general\\\\4_imageNet\\\\tiny-imagenet-200\\\\\"\n",
    "\n",
    "# load_data=TinyImageNetLoader(\"val\",data_path=path)\n",
    "# x_data, y_data= load_data.load_n_images_all_classes(nb=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
